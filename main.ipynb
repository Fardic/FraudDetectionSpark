{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6648ea3d",
   "metadata": {
    "papermill": {
     "duration": 0.023724,
     "end_time": "2022-04-18T21:29:22.094306",
     "exception": false,
     "start_time": "2022-04-18T21:29:22.070582",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Logistic Regression Model for Fraud Detection using Pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b160ff9",
   "metadata": {
    "papermill": {
     "duration": 0.023054,
     "end_time": "2022-04-18T21:29:22.140101",
     "exception": false,
     "start_time": "2022-04-18T21:29:22.117047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Summary\n",
    "In this notebook, I would like to share my humble pyspark \"skills\" with anyone interested. Wanted to use Logistic Regression as the first model and I was going to investigate other models as well. However, at first try, model accuracy reached 100% so I stopped working on the data. Let's get it started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57345511",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T21:29:22.188606Z",
     "iopub.status.busy": "2022-04-18T21:29:22.188223Z",
     "iopub.status.idle": "2022-04-18T21:30:10.529077Z",
     "shell.execute_reply": "2022-04-18T21:30:10.527937Z"
    },
    "papermill": {
     "duration": 48.36954,
     "end_time": "2022-04-18T21:30:10.532682",
     "exception": false,
     "start_time": "2022-04-18T21:29:22.163142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\r\n",
      "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l-\b \b\\\b \bdone\r\n",
      "\u001b[?25hCollecting py4j==0.10.9.3\r\n",
      "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.0/199.0 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: pyspark\r\n",
      "  Building wheel for pyspark (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \bdone\r\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=78f7d38a9b478f5069248c90a24e4199d487a244bd4dd55eba633eb88f6f733e\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\r\n",
      "Successfully built pyspark\r\n",
      "Installing collected packages: py4j, pyspark\r\n",
      "  Attempting uninstall: py4j\r\n",
      "    Found existing installation: py4j 0.10.9.5\r\n",
      "    Uninstalling py4j-0.10.9.5:\r\n",
      "      Successfully uninstalled py4j-0.10.9.5\r\n",
      "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "import numpy as np\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.sql.types import FloatType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c9b571",
   "metadata": {
    "papermill": {
     "duration": 0.170241,
     "end_time": "2022-04-18T21:30:10.875825",
     "exception": false,
     "start_time": "2022-04-18T21:30:10.705584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Investigation\n",
    "\n",
    "Let's see what kind of features we have in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "633c29ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T21:30:11.219440Z",
     "iopub.status.busy": "2022-04-18T21:30:11.219201Z",
     "iopub.status.idle": "2022-04-18T21:30:29.475143Z",
     "shell.execute_reply": "2022-04-18T21:30:29.474496Z"
    },
    "papermill": {
     "duration": 18.429583,
     "end_time": "2022-04-18T21:30:29.476952",
     "exception": false,
     "start_time": "2022-04-18T21:30:11.047369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/conda/lib/python3.7/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/04/18 21:30:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.enabled\", True)# Property used to format output tables better\n",
    "\n",
    "df = spark.read.csv(\"../input/online-payments-fraud-detection-dataset/PS_20174392719_1491204439457_log.csv\", inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bc76aab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T21:30:29.784001Z",
     "iopub.status.busy": "2022-04-18T21:30:29.783737Z",
     "iopub.status.idle": "2022-04-18T21:30:31.306204Z",
     "shell.execute_reply": "2022-04-18T21:30:31.305676Z"
    },
    "papermill": {
     "duration": 1.699305,
     "end_time": "2022-04-18T21:30:31.307858",
     "exception": false,
     "start_time": "2022-04-18T21:30:29.608553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6362620"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Length of the whole dataset\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cfab5e",
   "metadata": {
    "papermill": {
     "duration": 0.108885,
     "end_time": "2022-04-18T21:30:31.526628",
     "exception": false,
     "start_time": "2022-04-18T21:30:31.417743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We see that the dataset consist of mostly numerical values. However, we should investigate string features to see if there is any column that can be helpful for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ec4274",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T21:30:31.813314Z",
     "iopub.status.busy": "2022-04-18T21:30:31.813046Z",
     "iopub.status.idle": "2022-04-18T21:30:31.824293Z",
     "shell.execute_reply": "2022-04-18T21:30:31.823532Z"
    },
    "papermill": {
     "duration": 0.191013,
     "end_time": "2022-04-18T21:30:31.827071",
     "exception": false,
     "start_time": "2022-04-18T21:30:31.636058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- step: integer (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- amount: double (nullable = true)\n",
      " |-- nameOrig: string (nullable = true)\n",
      " |-- oldbalanceOrg: double (nullable = true)\n",
      " |-- newbalanceOrig: double (nullable = true)\n",
      " |-- nameDest: string (nullable = true)\n",
      " |-- oldbalanceDest: double (nullable = true)\n",
      " |-- newbalanceDest: double (nullable = true)\n",
      " |-- isFraud: integer (nullable = true)\n",
      " |-- isFlaggedFraud: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd00a2",
   "metadata": {
    "papermill": {
     "duration": 0.124999,
     "end_time": "2022-04-18T21:30:32.076450",
     "exception": false,
     "start_time": "2022-04-18T21:30:31.951451",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Seems like type may be used since there may be a relation between payment type and probability of transaction to be fraud. We also have nameOrig and nameDest columns which are anonymized. They may be useful since fraud may be caused by the customer or a place that has higher fraud rates compared to others. I do not want to start with a complicated model so I will look at the number of distinct values for these columns. If they are small enough, I will add them in my features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e38a271",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T21:30:32.312643Z",
     "iopub.status.busy": "2022-04-18T21:30:32.312098Z",
     "iopub.status.idle": "2022-04-18T21:30:32.598021Z",
     "shell.execute_reply": "2022-04-18T21:30:32.597298Z"
    },
    "papermill": {
     "duration": 0.400978,
     "end_time": "2022-04-18T21:30:32.600076",
     "exception": false,
     "start_time": "2022-04-18T21:30:32.199098",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|step|    type|  amount|   nameOrig|oldbalanceOrg|newbalanceOrig|   nameDest|oldbalanceDest|newbalanceDest|isFraud|isFlaggedFraud|\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "|   1| PAYMENT| 9839.64|C1231006815|     170136.0|     160296.36|M1979787155|           0.0|           0.0|      0|             0|\n",
      "|   1| PAYMENT| 1864.28|C1666544295|      21249.0|      19384.72|M2044282225|           0.0|           0.0|      0|             0|\n",
      "|   1|TRANSFER|   181.0|C1305486145|        181.0|           0.0| C553264065|           0.0|           0.0|      1|             0|\n",
      "|   1|CASH_OUT|   181.0| C840083671|        181.0|           0.0|  C38997010|       21182.0|           0.0|      1|             0|\n",
      "|   1| PAYMENT|11668.14|C2048537720|      41554.0|      29885.86|M1230701703|           0.0|           0.0|      0|             0|\n",
      "+----+--------+--------+-----------+-------------+--------------+-----------+--------------+--------------+-------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70c7b69",
   "metadata": {
    "papermill": {
     "duration": 0.111193,
     "end_time": "2022-04-18T21:30:32.823724",
     "exception": false,
     "start_time": "2022-04-18T21:30:32.712531",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Type has only 5 distinct values so it will not create too much columns when one hot encoding is implemented. Therefore, I will include Type column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "534ae2c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T21:30:33.140197Z",
     "iopub.status.busy": "2022-04-18T21:30:33.139979Z",
     "iopub.status.idle": "2022-04-18T21:30:43.099717Z",
     "shell.execute_reply": "2022-04-18T21:30:43.098832Z"
    },
    "papermill": {
     "duration": 10.120739,
     "end_time": "2022-04-18T21:30:43.103372",
     "exception": false,
     "start_time": "2022-04-18T21:30:32.982633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>type</th></tr>\n",
       "<tr><td>TRANSFER</td></tr>\n",
       "<tr><td>CASH_IN</td></tr>\n",
       "<tr><td>CASH_OUT</td></tr>\n",
       "<tr><td>PAYMENT</td></tr>\n",
       "<tr><td>DEBIT</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+--------+\n",
       "|    type|\n",
       "+--------+\n",
       "|TRANSFER|\n",
       "| CASH_IN|\n",
       "|CASH_OUT|\n",
       "| PAYMENT|\n",
       "|   DEBIT|\n",
       "+--------+"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"type\").distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9fba3c",
   "metadata": {
    "papermill": {
     "duration": 0.235515,
     "end_time": "2022-04-18T21:30:43.579517",
     "exception": false,
     "start_time": "2022-04-18T21:30:43.344002",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "nameDest and nameOrig has lots of unique values so, for now, I will discard them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8155d574",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T21:30:43.947343Z",
     "iopub.status.busy": "2022-04-18T21:30:43.947097Z",
     "iopub.status.idle": "2022-04-18T21:31:07.304327Z",
     "shell.execute_reply": "2022-04-18T21:31:07.303282Z"
    },
    "papermill": {
     "duration": 23.546001,
     "end_time": "2022-04-18T21:31:07.310967",
     "exception": false,
     "start_time": "2022-04-18T21:30:43.764966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2722362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6353307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(df.select(\"nameDest\").distinct().count())\n",
    "print(df.select(\"nameOrig\").distinct().count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2696c87a",
   "metadata": {
    "papermill": {
     "duration": 0.185721,
     "end_time": "2022-04-18T21:31:07.724053",
     "exception": false,
     "start_time": "2022-04-18T21:31:07.538332",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I also want to look at the step data which is called as a time unit. I believe these transactions happened in 743. To be honest, I was expecting this column to be hours in the day. I am not sure if this column will be important but it does not need much attention so I include this column as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c07a50d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T21:31:08.032692Z",
     "iopub.status.busy": "2022-04-18T21:31:08.032364Z",
     "iopub.status.idle": "2022-04-18T21:31:12.892939Z",
     "shell.execute_reply": "2022-04-18T21:31:12.892275Z"
    },
    "papermill": {
     "duration": 4.98288,
     "end_time": "2022-04-18T21:31:12.894591",
     "exception": false,
     "start_time": "2022-04-18T21:31:07.911711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|step|\n",
      "+----+\n",
      "| 743|\n",
      "| 742|\n",
      "| 741|\n",
      "| 740|\n",
      "| 739|\n",
      "+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select(\"step\").distinct().orderBy(\"step\", ascending=False).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3fa38d",
   "metadata": {
    "papermill": {
     "duration": 0.164412,
     "end_time": "2022-04-18T21:31:13.205025",
     "exception": false,
     "start_time": "2022-04-18T21:31:13.040613",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Simple Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36234184",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T21:31:13.582062Z",
     "iopub.status.busy": "2022-04-18T21:31:13.581821Z",
     "iopub.status.idle": "2022-04-18T21:31:13.770783Z",
     "shell.execute_reply": "2022-04-18T21:31:13.770114Z"
    },
    "papermill": {
     "duration": 0.379825,
     "end_time": "2022-04-18T21:31:13.774316",
     "exception": false,
     "start_time": "2022-04-18T21:31:13.394491",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+-------------+--------------+--------------+--------------+-------+\n",
      "|step|    type|  amount|oldbalanceOrg|newbalanceOrig|oldbalanceDest|newbalanceDest|isFraud|\n",
      "+----+--------+--------+-------------+--------------+--------------+--------------+-------+\n",
      "|   1| PAYMENT| 9839.64|     170136.0|     160296.36|           0.0|           0.0|      0|\n",
      "|   1| PAYMENT| 1864.28|      21249.0|      19384.72|           0.0|           0.0|      0|\n",
      "|   1|TRANSFER|   181.0|        181.0|           0.0|           0.0|           0.0|      1|\n",
      "|   1|CASH_OUT|   181.0|        181.0|           0.0|       21182.0|           0.0|      1|\n",
      "|   1| PAYMENT|11668.14|      41554.0|      29885.86|           0.0|           0.0|      0|\n",
      "+----+--------+--------+-------------+--------------+--------------+--------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select(\"step\", \"type\", \"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\", \"isFraud\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6642838d",
   "metadata": {
    "papermill": {
     "duration": 0.188659,
     "end_time": "2022-04-18T21:31:14.165320",
     "exception": false,
     "start_time": "2022-04-18T21:31:13.976661",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Before going in to create a pipeline, we should split the data into train and test for investigation of model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4adbce06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T21:31:14.545927Z",
     "iopub.status.busy": "2022-04-18T21:31:14.545649Z",
     "iopub.status.idle": "2022-04-18T21:31:14.570973Z",
     "shell.execute_reply": "2022-04-18T21:31:14.570113Z"
    },
    "papermill": {
     "duration": 0.217788,
     "end_time": "2022-04-18T21:31:14.573325",
     "exception": false,
     "start_time": "2022-04-18T21:31:14.355537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train, test = df.randomSplit([0.7, 0.3], seed=5624)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cfde6a",
   "metadata": {
    "papermill": {
     "duration": 0.187738,
     "end_time": "2022-04-18T21:31:14.950134",
     "exception": false,
     "start_time": "2022-04-18T21:31:14.762396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We only have one string column to process so it is easy to handle it manually. Below code creates a pipeline that changes string value into numbers and then implements one hot encoder. After that, pipeline inserts the new features into the dataset. \n",
    "\n",
    "Note that we should include StandardScaler in this pipeline as well since we are going to use Logistic Regression as our model. Without the standardization, number of iteration may be increased and it would be hard for model to find global minima. However, I first wanted to try without standardization and it works as well. Just takes a little time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ec93856",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T21:31:15.326932Z",
     "iopub.status.busy": "2022-04-18T21:31:15.326642Z",
     "iopub.status.idle": "2022-04-18T21:31:15.360719Z",
     "shell.execute_reply": "2022-04-18T21:31:15.360194Z"
    },
    "papermill": {
     "duration": 0.227741,
     "end_time": "2022-04-18T21:31:15.363399",
     "exception": false,
     "start_time": "2022-04-18T21:31:15.135658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "string_indexer = [StringIndexer(inputCol=\"type\",\n",
    "                                outputCol=\"type\" + \"_StringIndexer\",\n",
    "                                handleInvalid=\"skip\")]\n",
    "                        \n",
    "one_hot_encoder = [OneHotEncoder(inputCols=[\"type_StringIndexer\"],\n",
    "                                 outputCols=[\"type_OneHotEncoder\"])]\n",
    "\n",
    "assemblerInput = [\"step\", \"amount\", \"oldbalanceOrg\", \"newbalanceOrig\", \"oldbalanceDest\", \"newbalanceDest\", \"isFraud\", \"type_OneHotEncoder\"]\n",
    "\n",
    "\n",
    "vector_assembler = VectorAssembler(inputCols=assemblerInput,\n",
    "                                   outputCol=\"VectorAssembler_features\")\n",
    "                    \n",
    "stages = []\n",
    "stages += string_indexer\n",
    "stages += one_hot_encoder\n",
    "stages += [vector_assembler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f89c0ed",
   "metadata": {
    "papermill": {
     "duration": 0.119989,
     "end_time": "2022-04-18T21:31:15.615719",
     "exception": false,
     "start_time": "2022-04-18T21:31:15.495730",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now lets process the data with our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "266993f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T21:31:15.850763Z",
     "iopub.status.busy": "2022-04-18T21:31:15.850521Z",
     "iopub.status.idle": "2022-04-18T21:31:37.204247Z",
     "shell.execute_reply": "2022-04-18T21:31:37.203450Z"
    },
    "papermill": {
     "duration": 21.473561,
     "end_time": "2022-04-18T21:31:37.206580",
     "exception": false,
     "start_time": "2022-04-18T21:31:15.733019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(11,[0,1,2,3,4,9]...|    0|\n",
      "|[1.0,484.57,54224...|    0|\n",
      "|[1.0,783.31,81503...|    0|\n",
      "|[1.0,863.08,92907...|    0|\n",
      "|[1.0,1076.27,3538...|    0|\n",
      "|[1.0,1271.77,6973...|    0|\n",
      "|(11,[0,1,2,3,4,9]...|    0|\n",
      "|[1.0,2643.45,6434...|    0|\n",
      "|[1.0,2673.64,7688...|    0|\n",
      "|[1.0,5763.99,1276...|    0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline().setStages(stages)\n",
    "pipe_model = pipeline.fit(train)\n",
    "\n",
    "train_data_pipe = pipe_model.transform(train)\n",
    "test_data_pipe = pipe_model.transform(test)\n",
    "\n",
    "train_data = train_data_pipe.select(F.col(\"VectorAssembler_features\").alias(\"features\"),\n",
    "                                    F.col(\"isFraud\").alias(\"label\"))\n",
    "\n",
    "test_data = test_data_pipe.select(F.col(\"VectorAssembler_features\").alias(\"features\"),\n",
    "                                  F.col(\"isFraud\").alias(\"label\"))\n",
    "\n",
    "train_data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23cf1da",
   "metadata": {
    "papermill": {
     "duration": 0.124262,
     "end_time": "2022-04-18T21:31:37.521694",
     "exception": false,
     "start_time": "2022-04-18T21:31:37.397432",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Logistic Regression  Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c887de5",
   "metadata": {
    "papermill": {
     "duration": 0.117242,
     "end_time": "2022-04-18T21:31:37.756869",
     "exception": false,
     "start_time": "2022-04-18T21:31:37.639627",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It's time to train our model and check the performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f77778a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T21:31:37.997591Z",
     "iopub.status.busy": "2022-04-18T21:31:37.997321Z",
     "iopub.status.idle": "2022-04-18T21:33:47.485080Z",
     "shell.execute_reply": "2022-04-18T21:33:47.484358Z"
    },
    "papermill": {
     "duration": 129.610452,
     "end_time": "2022-04-18T21:33:47.488039",
     "exception": false,
     "start_time": "2022-04-18T21:31:37.877587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/18 21:32:17 WARN MemoryStore: Not enough space to cache rdd_94_0 in memory! (computed 65.0 MiB so far)\n",
      "22/04/18 21:32:17 WARN BlockManager: Persisting block rdd_94_0 to disk instead.\n",
      "22/04/18 21:32:24 WARN BlockManager: Asked to remove block broadcast_66, which does not exist\n",
      "22/04/18 21:33:25 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_138_1 in memory.\n",
      "22/04/18 21:33:25 WARN MemoryStore: Not enough space to cache rdd_138_1 in memory! (computed 384.0 B so far)\n",
      "22/04/18 21:33:25 WARN BlockManager: Block rdd_138_1 could not be removed as it was not found on disk or in memory\n",
      "22/04/18 21:33:25 WARN BlockManager: Putting block rdd_138_1 failed\n",
      "22/04/18 21:33:25 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_138_3 in memory.\n",
      "22/04/18 21:33:25 WARN MemoryStore: Not enough space to cache rdd_138_3 in memory! (computed 384.0 B so far)\n",
      "22/04/18 21:33:25 WARN BlockManager: Block rdd_138_3 could not be removed as it was not found on disk or in memory\n",
      "22/04/18 21:33:25 WARN BlockManager: Putting block rdd_138_3 failed\n",
      "22/04/18 21:33:25 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_138_2 in memory.\n",
      "22/04/18 21:33:25 WARN MemoryStore: Not enough space to cache rdd_138_2 in memory! (computed 384.0 B so far)\n",
      "22/04/18 21:33:25 WARN BlockManager: Block rdd_138_2 could not be removed as it was not found on disk or in memory\n",
      "22/04/18 21:33:25 WARN BlockManager: Putting block rdd_138_2 failed\n",
      "22/04/18 21:33:25 WARN MemoryStore: Failed to reserve initial memory threshold of 1024.0 KiB for computing block rdd_138_0 in memory.\n",
      "22/04/18 21:33:25 WARN MemoryStore: Not enough space to cache rdd_138_0 in memory! (computed 384.0 B so far)\n",
      "22/04/18 21:33:25 WARN BlockManager: Block rdd_138_0 could not be removed as it was not found on disk or in memory\n",
      "22/04/18 21:33:25 WARN BlockManager: Putting block rdd_138_0 failed\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Area Under ROC: 0.9996301888217659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 65:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "lr_spark = LogisticRegression().fit(train_data)\n",
    "\n",
    "print(f\"Training Area Under ROC: {lr_spark.summary.areaUnderROC}\")\n",
    "print(f\"Training Accuracy: {lr_spark.summary.accuracy}\")\n",
    "pred = lr_spark.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16e26d11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T21:33:47.797692Z",
     "iopub.status.busy": "2022-04-18T21:33:47.797403Z",
     "iopub.status.idle": "2022-04-18T21:34:30.888514Z",
     "shell.execute_reply": "2022-04-18T21:34:30.888009Z"
    },
    "papermill": {
     "duration": 43.238165,
     "end_time": "2022-04-18T21:34:30.892762",
     "exception": false,
     "start_time": "2022-04-18T21:33:47.654597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.9996125781729113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"Test Area Under ROC: {pred.areaUnderROC}\")\n",
    "print(f\"Test Accuracy: {pred.accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ef09402",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T21:34:31.354319Z",
     "iopub.status.busy": "2022-04-18T21:34:31.354081Z",
     "iopub.status.idle": "2022-04-18T21:34:40.527102Z",
     "shell.execute_reply": "2022-04-18T21:34:40.526444Z"
    },
    "papermill": {
     "duration": 9.392723,
     "end_time": "2022-04-18T21:34:40.529415",
     "exception": false,
     "start_time": "2022-04-18T21:34:31.136692",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pyspark/sql/context.py:127: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  FutureWarning\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "metrics = MulticlassMetrics(pred.predictions.select(\"prediction\", \"label\").withColumn(\"label\", F.col(\"prediction\").cast(FloatType())).rdd.map(tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8aeae873",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T21:34:40.962514Z",
     "iopub.status.busy": "2022-04-18T21:34:40.962213Z",
     "iopub.status.idle": "2022-04-18T21:35:05.016229Z",
     "shell.execute_reply": "2022-04-18T21:35:05.015651Z"
    },
    "papermill": {
     "duration": 24.276556,
     "end_time": "2022-04-18T21:35:05.018033",
     "exception": false,
     "start_time": "2022-04-18T21:34:40.741477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "cm = metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce622cb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-18T21:35:05.295954Z",
     "iopub.status.busy": "2022-04-18T21:35:05.295697Z",
     "iopub.status.idle": "2022-04-18T21:35:05.301064Z",
     "shell.execute_reply": "2022-04-18T21:35:05.300481Z"
    },
    "papermill": {
     "duration": 0.146409,
     "end_time": "2022-04-18T21:35:05.303738",
     "exception": false,
     "start_time": "2022-04-18T21:35:05.157329",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1906191.,       0.],\n",
       "       [      0.,    2453.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abd05af",
   "metadata": {
    "papermill": {
     "duration": 0.206874,
     "end_time": "2022-04-18T21:35:05.729953",
     "exception": false,
     "start_time": "2022-04-18T21:35:05.523079",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "100% Test Accuracy, Not much to say.. Maybe cross validation code can be implemented to see a better approximation to real performance but with 6 million data, that many accuracy is hard to be a coincidence. Anyway, I would like to see more analysis on the data as well.(Especially visualization using pyspark would be really helpful.) Thanks for reading!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2cb6d8",
   "metadata": {
    "papermill": {
     "duration": 0.135159,
     "end_time": "2022-04-18T21:35:06.002300",
     "exception": false,
     "start_time": "2022-04-18T21:35:05.867141",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fatih Özgür Ardıç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f708429f",
   "metadata": {
    "papermill": {
     "duration": 0.136477,
     "end_time": "2022-04-18T21:35:06.274491",
     "exception": false,
     "start_time": "2022-04-18T21:35:06.138014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 357.460929,
   "end_time": "2022-04-18T21:35:09.032401",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-18T21:29:11.571472",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
